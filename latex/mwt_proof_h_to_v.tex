%mwt_proof_h_to_v.tex

\subsection{H-Polyhedra $\to$ V-Polyhedra}
\paragraph{Overview}
As Figure \ref{fig:proof} indicates, the proof in this direction will generally be of the following form:
  \[ \PH \to \CH \to \CVp \to \CV \to \PV \]
It should then be of no surprise that the actual proof is essentially just elucidating this diagram.

\subsubsection{Relax: $\PH \to \CH$} 
This is a straightforward step.  Let $\PH = \{x : A\x \leq \b\}$ for some $A$ and $\b$.  To achieve the form of a cone, the right hand side of the inequality needs to be $0$.  In order to do this, we prepend $\b$ to the matrix $A$, and introduce a new variable as follows:
  \[ A\x \leq \b \to [\b|A] \xx \leq 0 \]
Observe that this is in fact a relaxation to an H-Cone.  The restriction back to the original H-Polyhedron is to intersect the H-Cone with the hyperplane $\{\x : x_0 = 1\}$, that is:
  \[ \PH = \left\{ [\b|A] \xx \leq 0 \right\} \cap \{ \x : x_0 = 1 \} \]
For now, we deal with the relaxed cone $\CH$ and do this restriction as the last step in the proof of this direction.  In what follows, $A' = [\b|A]$ and $\x' = \xx$.  This notation will simplify the expressions that follow.

\subsubsection{Lift: $\CH \to \CVp$}
The task is to \textit{somehow} go from dealing with halfspaces to dealing with rays.  This step is not exactly straighforward, however the technique used here is systematic from the point of view of linear programming.  Let $A^i$ denote the $i$-th column of $A$.  Observe that $A\x = \sum_i A^i\cdot x_i$.  The significance of this is that matrix multiplication can be considered in two fundamentally different ways, as either: a number of dot products $\dotproduct{\a_i}{\x}$, or as a linear combination of column vectors $\sum_i A^i\cdot x_i$.  This important difference is key to representing the inequality above as a cone.  Suppose that $A \in \R^{m\times n}$, i.e. that $A$ as $m$ rows, and consider the vector $x_i\cdot\smallstack{\e_i}{A^i} \in \R^{n + m}$.  Here, the first $n$ rows keep track of the \textit{value} and \textit{position} of the contribution of $x_i$ (a scalar) to the vector $\x$, while the bottom $m$ rows correspond to the contribution of $x_i$ to the \textit{sum} $\sum_i A^i\cdot x_i$.

Let $\pi_{\x} : \R^{n+m} \to \R^{n}$ be a projection to the first $n$ coordinates, and $\pi^{\x} : \R^{n+m} \to \R^{m}$ a projection to the last $m$ coordinates.  Now, consider the sum 
  \[ \sum\nolimits_i x_i \stack{\e_i}{A^i} = \stack{\x}{A\x} \]
This equality follows from the discussion that precedes it, and it is quite significant, mostly because:
  \[ \pi_{\x} \stack{\x}{A\x}\ = \x,\quad \pi^{\x} \stack{\x}{A\x}\ = A\x \]
\todo{I think that switching the notation of ``projection to'' and ``projection from'' would be in order}
Without further adieu, consider the cone:
  \[ \C_0 = \cone\left(\left\{\pm\stack{\e_i}{A^i}\right\}\right) \]
What are the members of this cone?  Say $\smallstack{\x}{\z} \in \C_0$, then this vector must be a positive linear combination of its generators \todo{define generators}.  In other words:
  \[ \exists\, t^+_i, t^-_i \geq 0 \left| \stack{\x}{\z} = 
       \sum\nolimits_i t^+_i \cdot\stack{\e_i}{A^i} + 
                       t^-_i \cdot{-\!\!\stack{\e_i}{A^i}} \right. =
       \sum\nolimits_i (t^+_i - t^-_i) \cdot\stack{\e_i}{A^i}
   \]
Letting $t_i = (t^+_i - t^-_i)$ in the above equations, and noting that $t_i$ can range over all of $\R$, we get:
  \[ \exists t_i \in \R \left| \stack{\x}{\z} = \sum\nolimits_i t_i \cdot\stack{\e_i}{A^i} \right.
       \Rightarrow \z = A\x \]
To recap what has been accomplished, we have created a V-Cone, in which the first $n$ coordinates keep track of some values corresponding to a vector we've been calling $\x$, and the final $m$ coordinates keep track of the result of the multiplication $A\x$.  This V-Cone becomes powerful when we fix $\z$, that is, taking the V-Cone's intersection with a set of the form $\smallsetzeq{\z}{\b}$ for some carefully chosen vector $\b$.  Then, we take a projection of this new set formed by intersection, and recover useful values of $\x$, that is:
  \[ \pi_{\x}\left(\C_0 \cap 
             \setzeq{\z}{\b} \right) =
     \left\{\x \suchthat A\x = \b\right\} 
  \]
In this way, we can recover the solutions to the equation $A\x = \b$.  As previously mentioned, the projection of a V-Cone is easy to deal with, just restrict the generators to the desired coordinates, but the intersection is a bit trickier to deal with, and is the subject of the next part of the proof.  There is one more loose end to tie up before we go on.

As of right now, we have an alleged way to get the solutions of $A\x = \b$ from a V-Cone, but what we really need are the solutions to $A\x \leq \b$.  There is a quick and dirty way to get this from $\C_0$, through introducing what are known in linear programming circles as \textit{slack variables.}  This is just a form of relaxation.

Suppose that you have an $\x$ such that $A\x \leq \b$, and let $\w = \b - A\x$.  This $\w$ can be thought of as \textit{slack}, and is the key to getting from $\C_0$ to $\CVp$.  The key property of this slack is $0 \leq \w$, so $\w$ can be written as
  \[ \w = \sum\nolimits_j w_j\cdot\e_j 
          \Rightarrow \exists\,w_j \geq 0 \;\big|\; \w = \sum\nolimits_j w_j\cdot\e_j \]
Now, we add these bases vectors $\e_j$ to the generators of $\C_0$ to create $\CVp$:
  \[ \CVp = \cone\left(\left\{\pm\stack{\e_i}{A^i}\right\} \cup 
                 \left\{\stack{\vec{0}}{\e_j}\right\}\right) 
  \]
As before, we fix the last $m$ coordinates, carefully choosing our $\b$ to be $\vec{0}$, and again projecting to the coordinates that interest us:
  \[ \pi_{\x} \left( \CVp \cap \setzeq{\z}{\vec{0}} \right) = 
               \{\x \suchthat A\x \leq \vec{0}\}
  \]

\subsubsection{Drop: $\CVp \to \CV$}
Now we turn to intersecting $\CVp$ with $\smallsetzeq{\z}{\vec{0}}$.  This requires a pretty good idea, and is again not straightforward.  Remembering that $\z \in \R^m$, and letting $M = \{1,2,\dots,m\}$, the first step is to note that:
  \[ \setzeq{\z}{\vec{0}} = 
     \bigcap_{k \in M} \setzeq{z_k}{0} \]
This suggests that, instead of taking $\CVp$ and intersecting it with $\smallsetzeq{\z}{\vec{0}}$ all at once, we intersect it with $\smallsetzeq{z_k}{0}$ for each $k \in M$ one at a time.  This greatly simplifies our task of creating a cone to represent ${\CVp \cap \smallsetzeq{\z}{\vec{0}}}$ to finding a cone that represents ${\CVp \cap \smallsetzeq{z_k}{0}}$, and applying this construction inductively.

The basic idea is as follows.  Say we are given to numbers $x$ and $y$, and the equation: ${\alpha x - \beta y = 0 }$, where we are to choose $\alpha$ and $\beta$ so as to satisfy it.  One obvious choice would be to pick ${\alpha = \beta = 0}$.  This choice is pretty trivial, and doesn't really say anything about $x$ or $y$.  Another, slightly more clever choice, would be $\alpha = y$ and $\beta = x$.  

Now say we have two vectors $\x$ and $\y$, such that $x_k > 0$ and $y_k < 0$, and are given the expression $\alpha \x + \beta \y$, where we are to choose $\alpha \geq 0$ and $\beta \geq 0$ such that the resulting vector is $0$ in the $k$-th coordinate.  Again, $\alpha = \beta = 0$ works, but it isn't too helpful.  A more useful choice would be $\alpha = -y_k$ and $\beta = x_k$.

Now to intersect our cone $\CVp$ with $\smallsetzeq{z_k}{0}$.  The first step is to partition the generators of $\CVp$.  Say $\CVp$ is given by $\CVp = \cone(\U)$.  We want to partition $\U$ into three sets, depending on each vector's value at $z_k$.  In particular, let
  \begin{align*} \Z &= \{\u \in \U \suchthat u_k = 0\} \\
                 \P &= \{\u \in \U \suchthat u_k > 0\} \\
                 \N &= \{\u \in \U \suchthat u_k < 0\} 
  \end{align*}
\todo{Here there is a good chance to use Minkowski sums, and they may make some of the reasoning more clear and concise}  These sets \textit{partition} $\U$, i.e. $\U = \Z \cup \P \cup \N$, and each $\Z, \P, \N$ are pairwise disjoint.  This means, for any vector $\v \in \cone(\U)$, we have:
  \[ \v = \setsum{t}{l}{\Z} +
          \setsum{t}{i}{\P} +
          \setsum{t}{j}{\N}
  \] 
Note that the superscript notation $\u^i$ is merely meant to index the vectors.  This makes it easier to refer to the $k$-th coordinate of the $i$-th vector from $\P$ as $u^i_k$.  What we would like to have is $\v$ expressed in terms of vectors whose $k$-th coordinate is already $0$, so we need to construct these vectors from $\Z$, $\P$, and $\N$.  First note that the vectors in $\Z$ already have this property, so we can forget about them for a second.  Next, suppose that $v_k = 0$, and consider the contribution from ${\setsum{t}{i}{\P} + \setsum{t}{j}{\N}}$ to the $k$-th coordinate.  We must have that:
  \[ \setsumk{t}{i}{\P} + \setsumk{t}{j}{\N} = 0
      \quad\Rightarrow\quad
     \setsumk{t}{i}{\P} = \setsumk{-t}{j}{\N}
  \]
This final equality will be the key to our new set of generators, and the sum will be denoted $\sigma$.  The next step is more easily expressed as equations than in words, so:
\begin{align*}
  & \setsum{t}{i}{\P} + \setsum{t}{j}{\N} \quad= \\[1em]
  & \frac{1}{\sigma}\left(\setsumk{-t}{j}{\N}\right)\setsum{t}{i}{\P} + 
      \frac{1}{\sigma}\left(\setsumk{t}{i}{\P}\right)\setsum{t}{j}{\N} \quad= \\[1em]
  & \frac{1}{\sigma}\sum_{\substack{i\in\P\\j\in\N}} -(t_i t_j)u^j_k\u^i + 
      \frac{1}{\sigma}\sum_{\substack{i\in\P\\j\in\N}} (t_i t_j)u^i_k\u^j \quad= \\[3pt]
  & \frac{1}{\sigma}\sum_{\substack{i\in\P\\j\in\N}} (t_i t_j) (u^i_k\u^j - u^j_k\u^i)
\end{align*}
All we've done here is multiply by $1$, separate, and combine terms, but the result is pretty neat.  Since $u^i_k\u^j - u^j_k\u^i$ has $0$ in the $k$-th coordinate, and $t_i t_j > 0$, what we have is the orginal contribution of vectors from $\P$ and $\N$ expressed as a positive combination of vectors with $0$ in the $k$-th coordinate, and this is precisely the property that we sought in the beginning.  Reconsidering our vector $\v$,
  \begin{align*} \v &= \setsum{t}{l}{\Z} + \setsum{t}{i}{\P} + \setsum{t}{j}{\N} = \\
                    &= \setsum{t}{l}{\Z} + 
      \frac{1}{\sigma}\sum_{\substack{i\in\P\\j\in\N}} (t_i t_j) (u^i_k\u^j - u^j_k\u^i)
  \end{align*}
If we let:
  \[\P \fjoink \N = \{u^i_k\u^j - u^j_k\u^i \;|\; \u^i \in \P,\, \u^j \in \N \}\]
and quickly set $H_k = \smallsetzeq{z_k}{0}$, then we see:
  \[ \cone(\U) \cap H_k = \cone(\Z \cup \P \cup \N) \cap H_k = 
                          \cone(\Z \cup \P \fjoink \N)
  \]
As was previously discussed, we can iterate this procedure over every $k \in M$, and in doing so take the set $\CVp \cap \smallsetzeq{\z}{\vec{0}}$ to a new cone expressed without intersection.  After we have this cone, we can project away the coordinates that are always $0$ with $\pi^{\x}$ to get a new cone $\CV$.  \todo{It may be worthwhile to give this procedure more explicitly, creating a new operator $\mathcal{F}$ that takes a set, and, for every $k$, partitions it on $k$ then $\fjoink$'s it}.

\subsubsection{Restrict: $\CV \to \PV$}
There's one last loose end to tie up to finish this direction of the proof.  When we relaxed $\PH$, we introduced a new coordinate, $x_0$, and now we need to intersect $\CV$ with $H_1 = \{\smallstack{x_0}{\x} | x_0 = 1\}$.  \todo{I may have forgotten about $x_0$, in particular when saying things like $\dots \in \R^{n+m}$}.  As before, suppose that $\CV = \cone(\U')$, and partition $\U'$ into $\Z$, $\N$, and $\P$.  Denote $\sum_{\u \in \P}u_0 = \sigma_p$.  It will be convenient to consider a ``normalized'' version of $\P$:
  \[ \V' = \left\{\frac{\u^i}{u^i_0} \suchthat \u^i \in \P\right\} \]
The choice of term ``normalized'' will be clear shortly.  In order to intersect our cone with $H_1$, we will first recognize that vectors from $\Z$ won't help us reach our hyperplane.  Likewise, any contribution from vectors of $\N$ will ``cancel out'' some contribution from $\P$.  With this in mind, consider again a vector $\v \in \CV \cap H_1$,
  \begin{align*}
  \v &= \setsum{t}{l}{\Z} +
        \setsum{t}{i}{\P} +
        \setsum{t}{j}{\N} \\[3pt]
   1 &= \setsumk{t}{i}{\P} + \setsumk{t}{j}{\N} \;\Rightarrow \\
   \sigma_p &= \setsumk{t}{i}{\P} = 1 - \setsumk{t}{j}{\N} \\
  \end{align*}
Similarly as before, we'll try to ``simplify'' the expression involving vectors from $\N$.
\begin{align*}
  & \setsum{t}{i}{\P} + \setsum{t}{j}{\N} \quad= \\[1em]
  & \frac{1}{\sigma_p}\left(1 - \setsumo{t}{j}{\N}\right)\setsum{t}{i}{\P} + 
      \frac{1}{\sigma_p}\left(\setsumo{t}{i}{\P}\right)\setsum{t}{j}{\N} \quad= \\[1em]
  & \frac{1}{\sigma_p}\sum_{i\in\P}t_i \u^i + 
      \frac{1}{\sigma_p}\sum_{\substack{i\in\P\\j\in\N}} -(t_i t_j)u^j_0\u^i + 
      \frac{1}{\sigma_p}\sum_{\substack{i\in\P\\j\in\N}} (t_i t_j)u^i_0\u^j \quad= \\[3pt]
  & \sum_{i\in\P}\left(\frac{t_i u^i_0}{\sigma_p}\right) \frac{\u^i}{u^i_0} + 
      \frac{1}{\sigma_p}\sum_{\substack{i\in\P\\j\in\N}} (t_i t_j) (u^i_0\u^j - u^j_0\u^i)
\end{align*}
First, note that the vectors in the second term all belong to $\P \fjoin{0} \N$ and contribute nothing to the $0$-th coordinate.  Now consider the sum $\sum_{i\in\P}\frac{t_i u^i_0}{\sigma_p}$ from the first term.  Because the $0$-th coordinate must sum to $1$, the second term contributes nothing, and every vector of the form $\frac{\u^i}{u^i_0}$ has value $1$ in the $0$-th position, it must be that $\sum_{i\in\P}\frac{t_i u^i_0}{\sigma_p} = 1$.  Furthermore, since $t_i, u^i_0, \sigma_p \geq 0$, $\sum_{i\in\P}\frac{t_i u^i_0}{\sigma_p} \frac{\u^i}{u^i_0}$ is actually a \textit{convex combination} of vectors from $\V'$!  Reconsidering the $\v$:
  \begin{align*}
  \v &= \setsum{t}{l}{\Z} +
        \setsum{t}{i}{\P} +
        \setsum{t}{j}{\N} \\[3pt]
     &= \sum_{i\in\P}\left(\frac{t_i u^i_0}{\sigma_p}\right) \frac{\u^i}{u^i_0} + 
        \setsum{t}{l}{\Z} +
        \frac{1}{\sigma_p}\sum_{\substack{i\in\P\\j\in\N}} (t_i t_j) (u^i_0\u^j - u^j_0\u^i)
  \end{align*}
What this shows is that any vector from $\CV \cap H_1$ can be written as the sum of vectors from the convex hull of $\V'$ and the conical hull of $\U'' = \Z \cup \fjoin{\P}{\N}$.  In symbols:
  \[ \CV \cap H_1 = \conv(\V') \oplus \cone(\U'') \]
The final step is to rid ourselves of the $0$-th coordinate with $\pi^0$:
  \begin{align*}
    \U  &= \pi^0(\U'') \\
    \V  &= \pi^0(\V') \\
    \PV &= \conv(\V) \oplus \cone(\U)
  \end{align*}
And we finally have our V-Polyhedron constructed from $\PH$ from the beginning of the proof.

\subsubsection{Summary}
\input{proof_diagram_h_to_v}
Here will be presented a terse summary of the proof we've just gone through (see Figure \ref{fig:h_to_v}).  There will be liberal abuse of notation.  First, we relax an H-Polyhedron $\{A\x \leq \b\}$ to an H-Cone by adding an extra variable $\x_0$ that ``scales'' the constraint $\b$.  This H-Cone is then lifted to an intersection of many hyperplanes of the form $\{\x_k = 0\}$.  This set is then expanded with the operator $\fjoink$ and projected down with $\pi^k$, one extraneous dimension at a time.  At the end, the cone $\CV$ is restricted back to the original Polyhedron with one last, modified application of $\fjoin{0}$ and $\pi^0$.  The final result is a Minkowski Sum of a cone and convex hull $\cone(U) \oplus \conv(V)$.  Calling the polyhedra $\PH$ and $\PV$ equivalent is justified because of the notion of polyhedra equivalence, and the fact that the projections used maintained a bijective equivalence with the original set.

% by arrow

