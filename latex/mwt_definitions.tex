
\subsection{Informal Notions}
% informal notions

Here shall be stated some of the fundamental ideas involved in the theory of polyhedra (formal definitions will be given in the next section).

\paragraph{Convexity} Convexity is an essential notion in the study of polyhedra.  Here, it will be considered a property of subsets of $\R^n$.  Let $A \subseteq \R^n$.  A is \textit{convex} if, given any two points of $A$, the line segment connecting these two points is also contained in $A$.  It formalizes a notion of being ``filled-in,'' and implies that (given the standard metric) the shortest path between any two points of $A$ is included in $A$.  

\paragraph{Convex Hulls} Associated with the notion of convexity is the operation of \textit{convex hull}.  Given any subset $B \subseteq \R^n$, the \textit{convex hull} of $B$ is the smallest set which contains $B$ and is convex.  For two isolated points, their convex hull is the line segment with the given points as endpoints.  For more complex sets, it can be thought of as in iterative procedure where the convex hull of every two points is added to the set, repeating this operation until no more points are added. \todo{a picture showing how the procedure must iterate}.

\paragraph{Minkowski Sums}  There are a number of ways to ``combine'' two sets together in a meaningful way.  The most common include union, intersection, and product (sometimes called direct sum).  Another, less common, may be disjoint union.  In a space with a binary operator $+$, you may also consider a translation of a set, given by adding an element to every member of a set.  The Minkowski Sum is an aggregated translation, where instead of translating a set with one element, you translate a set with every member of one set, and then take the union of all these translations.

%- Important transforms
\paragraph{Projections, Lifts, Relaxations, Restrictions}  These terms will be will be used informally in this paper, however when one is needed in a formal context it will be specified as necessary.  

\textit{Projections} are very simple operations, which basically cause one to ``ignore'' the coordinates of a set for a given index set of coordinates.  For example, $\pi: (x,y) \mapsto (x)$ is a projection from $\R^2$ to $\R$.  Projections are often given a subscript to indicate which dimensions are being ``projected to,'' or a superscript to indicate which dimensions are being ``projected away.''  A formal notation for linear projections is provided below.

A \textit{lift} refers to taking a set and representing it in a higher dimension.  For example, one may take the unit interval $I = [0,1] \subseteq \R^1$ and \textit{lift} it to $\R^2$ by considering $I \times {0}$.  

\textit{Relaxations and Restrictions}: A \textit{relaxation} of a set will typically be a way to let a set ``grow,'' however it will typically be in a ``controlled'' or ``reversible'' manner.  These ``reversals'' will be referred to as \textit{restrictions}.  Take, for instance, this relaxation of the unit interval $I = [0,1]$: 
  \[\bigcup_{t \geq 0} [0,t]\times t \quad=\quad 
    \{(x,y) \suchthat x \geq 0\} \cap \{(x,y) \suchthat y - x \geq 0\}\]  
From this relaxation (an ``infinite triangle'' of sorts) we may recover $I$ with a respective restriction, in this case by intersecting the relaxation with ${\{(x,y) \suchthat y = 1\}}$ and projecting it to the x axis.  Again, the terms \textit{relaxation} and \textit{restriction} will be used informally, while specific instances will be specified as needed.


\subsection{Formal Definitions}
Here are the formal defintions required for the statement of the theorem and its proof.  In all definitions, unless otherwise stated, $I$ represents and arbitray \textit{finite} index set, and $J$ is an \textit{arbitrary} index set.

% convex combination
\paragraph{Convex Combination} 
Let $\vec{x},\vec{y} \in \R^n,\, \lambda \in [0,1]$.  Then
  \[\convComb{\vec{x}}{\vec{y}}{\lambda}\]
is called a \textit{convex combination} of $\vec{x}$ and $\vec{y}$.
More generally, $\vec{x}_i \in \R^n$, $\lambda_i \in [0,1]$, and $\sum_i \lambda_i = 1$.  Then 
  \[\convCombb{\vec{x}}{\lambda}{i}\]
is called a \textit{convex combination} of $\vec{x}_i$.

% convex
\paragraph{Convex} 
Let $A \subseteq R^n$.  $A$ is by definition \textit{convex} if every convex combination of its members is again a member.  In symbols:
  \[\vec{x}_i \in A \Rightarrow
    \convCombb{\vec{x}}{\lambda}{i} \in A\]
Note: Let $(\forall j \in J) \, A_j \subseteq \R^n$ be convex, and $(\forall i\in I)\,\vec{x}_i, \in \bigcap_{j\in J}A_j$.  Then 
  \[(\forall j\in J)\; \convCombb{\vec{x}}{\lambda}{i} \in A_j \Rightarrow \convCombb{\vec{x}}{\lambda}{i} \in \bigcap_{j \in J} A_j\]  
 This implies that $\bigcap_{i \in I} A_i$ is also convex.  In other words, the property of being convex is closed under the operation of intersection.

% halfspace
\paragraph{Halfspace}
Let $\vec{y} \in \R^n,\, c \in \R$.  Then the set
  \[ \{\vec{x} \suchthat \dotproduct{\vec{x}}{\vec{y}} \leq c\} \]
is called a \textit{halfspace}.  In particular, $H_k$ shall denote the halfspace given by
  \[ \{\vec{x} \suchthat \dotproduct{\vec{x}}{\vec{e}_k} \leq 0 \} = 
     \{\vec{x} \suchthat x_k \leq 0 \} \]
Note:  Suppose that 
  $\dotproduct{\vec{x}}{\vec{y}} \leq c,\;$ 
  $\dotproduct{\vec{z}}{\vec{y}} \leq c,\;$ and
  $\lambda \in [0,1]$.  Then
  \[ \dotproduct{\convComb{\vec{x}}{\vec{z}}{\lambda}}{\vec{y}} = 
    \convComb{\dotproduct{\vec{x}}{\vec{y}}}
             {\dotproduct{\vec{z}}{\vec{y}}}
             {\lambda} \leq \convComb{\cdot c}{\cdot c}{\lambda} = c
  \]
  This means that halfspaces are \textit{convex}.

%hyperplane
\paragraph{Hyperplane}
Let $\vec{y} \in \R^n,\, c \in \R$.  Then the set
  \[ \{\x \in \R^n \suchthat \dotproduct{\x}{\vec{y}} = c\} \]
is called a \textit{hyperplane}.  This is the precisely the boundary of a halfspace.
Note:  A family of useful hyperplanes come from projections, i.e.
  \[ \{\x \in \R^n \suchthat \dotproduct{\x}{\vec{e}_i} = c\} = 
     \{\x \in \R^n \suchthat x_i = c\} \]

% H-polyhedron
\paragraph{H-polyhedron}
An \textit{H-polyhedron} is a finite intersection of halfspaces.  By the remark given under the definition of \textbf{convexity}, \textit{H-polyhedra are convex.}  Here is a useful way of ``writing down'' an H-polyhedron:
  \[ \{ \vec{x} \in \R^n \suchthat 
    \forall i:\, \dotproduct{\vec{x}}{\vec{a}_i} \leq b_i \} \]
If all the $\vec{a}_i$ are gathered into a matrix $A$ (where the $\vec{a}_i$ become the rows of $A$), this is sometimes also written as:
  \[ \{ \vec{x} \in \R^n \suchthat A\vec{x} \leq \vec{b} \} \]

% -convex-hull
\paragraph{Convex Hull}
Let $A \subseteq \R^n$.  $B$ is by definition the \textit{convex-hull} of $A$ if
  \[ B = \conv(A) = \{ \vec{x} \suchthat \exists \vec{a}_i \in A, \exists\lambda_i \in [0,1]: \sum_i \lambda_i = 1,\,\vec{x} = \convCombb{\vec{a}}{\lambda}{i} \} \]

Note: Another useful, equivalent definition of \textit{convex hull} can be given as:
\[ B' = \conv'(A) = \bigcap \{C \suchthat 
      A \subseteq C, \, C \text{ is convex}\} \]
This more clearly illustrates the idea of ``smallest convex set containing $A$.''  To see that these definitions are equivalent, first note that $B$ is convex, and $A \subseteq B$, so $B' \subseteq B$ (since $B$ will be included in the intersection).  Then note that if $C$ is convex and $A \subseteq C$, then any convex combination of members of $A$ must (by definition) be in $C$, so it will be in every such $C$ and therefore in their intersection, so $B' \subseteq B.$

% conical hull
\paragraph{Conical Hull}  Let $\cup_{i\in I} \vec{a}_i = A \subseteq \R^n$, then $B$ is by definition the \textit{conical hull} of $A$ if:
  \[ B = \cone(A) = \{ \vec{x}\in \R^n \suchthat\; \exists t_i \in \R^n \suchthat \; t_i \geq 0,\; \vec{x} = \sum_{i\in I} t_i\vec{a}_i \} \]

Note: A conical hull is convex.  Let $K,\, (\forall i\in I)\, J_i$ be finite index sets (i.e. $J_i$ is a collection of finite index sets, indexed by the finite set $I$), $(\forall i \in I)(j \in J_i): t_{i,j} \geq 0,\, \vec{a}_{i,j} \in A$, and, as usual, $\lambda_i \in [0,1],\, \sum_i \lambda_i = 1$.  Then
  \[ \sum\nolimits_i \lambda_i \left(\sum\nolimits_{j \in J_i} t_{i,j} \vec{a}_{i,j}\right) = \sum\nolimits_{i,j\in J_i} \lambda_i \cdot t_{i,j} \vec{a}_{i,j}\]
is a ``conical combination'' of vectors in $A$ (because $\lambda_i\cdot t_{i,j} \geq 0$), so every convex combination from $\cone(A)$ is again in $\cone(A)$.

% minkowski sum
\paragraph{Minkowski Sum}
Let $P,Q \subseteq \R^n$.  Then the \textit{Minkowski Sum} of $P$ and $Q$ is given by:
  \[ P \oplus Q = \{ p + q \suchthat p \in P,\, q \in Q \} \]

Note:  If both $P$ and $Q$ are convex, then so is $P \oplus Q$.  Indeed, let $p_i \in P,\, q_i \in Q$, then
  \[\sum_i\nolimits \lambda_i(p_i + q_i) = 
  \convCombb{p}{\lambda}{i} + \convCombb{q}{\lambda}{i} = 
  \bar{p} + \bar{q}
  \]
Where $\bar{p} \in P$ and $\bar{q} \in Q$ (because they are convex), so $\bar{p} + \bar{q} \in P \oplus Q$.

\paragraph{Linear Transforms}
Let $T : \R^n \to \R^n$ satisfy:
\begin{align*}  
  &\forall \x,\y \in \R^n, \alpha \in \R, \\
  &T(\alpha \x + \y) = \alpha T\x + T\y 
\end{align*}
Then $T$ is called a \textit{linear transform}.  By induction:
\[ T \left(\sum\nolimits_i \alpha_i \x_i\right) = 
           \sum\nolimits_i \alpha_i T(\x_i) \]

Note: Let $A, B \subseteq \R^n,\, \alpha_i \geq 0,\, \lambda_i \geq 0,\, \sum_i \lambda_i = 1$.  Then as a consequence of this definition,
\begin{align*} 
  T(A \oplus B) &= \{T(\x + \y) \suchthat \x \in A,\,\y \in B\} \\
                &= \{T(\x) + T(\y) \suchthat \x \in A,\,\y \in B\} \\
                &= T(A) \oplus T(B) \\
  T(\cone(A)) &= \left\{T\left(\sum\nolimits_i \alpha_i \a_i\right) 
                        \Suchthat \a_i \in A\right\} \\
              &= \left\{\sum\nolimits_i \alpha_i T(\a_i) 
                        \Suchthat \a_i \in A\right\} \\
              &= \cone(T(A)) \\
  T(\conv(A)) &= \left\{T\left(\sum\nolimits_i \lambda_i \a_i\right) 
                        \Suchthat \a_i \in A\right\} \\
              &= \left\{\sum\nolimits_i \lambda_i T(\a_i) 
                        \Suchthat \a_i \in A\right\} \\
              &= \conv(T(A))
\end{align*}

\paragraph{Linear Projections}
Let $\u \in \R^n$ be a unit vector, and $I \in \R^{n\times n}$ be the identity matrix.  Then
  \[ \pi^{\u} : \R^n \to \R^n = \x \mapsto \Proj{\u}\x \]
Is a \textit{linear projection induced by $\u$}.  It is so-called because it is a \textit{linear} transform: 
\[\pi^{\u}(\alpha \x + \y) = \alpha\pi^{\u}\x + \pi^{\u}\y\]
and a projection:
\[ \pi^{\u} \circ \pi^{\u} = \pi^{\u} \]
When $\u = \e_k$, then $\pi^{\e_k}$ is written $\pi^k$.  Another family of useful \textit{linear projections} are those given by
\[ \pi_k = I - \sum\nolimits_{i \neq k} \e_i\e_i^T \]
Note that: 
\[ \dotproduct{\e_j}{\pi_k \x} = \delta_{jk} x_j \]

Projections are discussed in more detail at the end of the paper.

% V-polyhedron
\paragraph{V-Polyhedron}  A \textit{V-Polyhedron} is a subset $\mathcal{P}$ of $\R^n$ given by 
  \[ \mathcal{P} = \cone(\mathcal{U}) \oplus \conv(\mathcal{V}) \] 
where $\mathcal{U}$ and $\mathcal{V}$ are both finite (possibly empty) subsets of $\R^n$.  By the remark given under the definition of Minkowski Sums, \textit{V-Polyhedra are convex}.

\bigskip
\noindent We are now prepared to state the \MWT.

% statement of theorem
\paragraph{\MWT}
Every V-Polyhedron is an H-Polyhedron, and every H-Polyhedron is a V-Polyhedron.  This permits the use of the term ``Polyhedron'' unambiguously.
\medskip

Note:  I've read that ``definitions should be hard and theorems should be easy.''  Perhaps there is a bit of this involved in the \MWT{}, but an unfortunate side effect is that the significance of a theorem may not be immediate.  The \MWT{} is a ``representation theorem,'' and is a consequence of a type of duality that exists between the V and H --type definitions of polyhedra.

For a quick demonstration of why a dual representation is important for these types of sets, consider the following two tasks:
\begin{itemize}
  \item Given a point $\vec{x}$, determine if it is a member of the set
  \item Produce an arbitrary member of the set
\end{itemize}
These two fundamental tasks tell completely different stories given one representation as opposed to the other.  For an H-Polyhedron, determining membership is straightforward and relatively inexpensive: for each halfspace constraint, check to see that it is fulfilled.  For a V-Polyhedron, you would need to either find a combination that produces the point, or provide a ``certificate'' that demonstrates that it is not in the set.  On the other hand, given an H-Polyhedron, producing a member of the set is not straightforward at all.  Given a list of half-spaces, it isn't even obvious (or necessarily cheap to determine) that their intersection is not empty!  A V-Polyhedron, on the other hand, is given by members of the set, so the task of providing a member (or determining if the set is empty or not) is already done for you.  Observe that these tasks are facilitated by the qualifiers used to describe the sets (the qualifier $\forall$ is used for the H-Polyhedron, while $\exists$ is used for the V-Polyhedron).

It should be noted that guaranteeing the existence of two representations does not promise an efficient method of creating one from the other.  The proof of the \MWT{} indicates an enumeration algorithm, and an implementation will be provided.  This enumeration can be inefficient, however.

% Equivalence of polyhedra
\subsection{Equivalence of Polyhedra}  The notion of equivalence is worth a small discussion.  Consider the two polyhedra $I = [0,1]$ and $I\times\{0\}$ (i.e. the unit interval in $\R^1$ and the unit interval in $\R^2$).  It would be unfortunate if the theory of polyhedra didn't consider these two entities as \textit{equivalent} in some manner.  The basic operation of equivalence in the theory is the \textit{affine transformation}.  Then polyhedra are considered equivalent if there is an affine transformation from one to the other, which is bijective \textit{when restricted to the second polyhedron}.  In our setting, it is enough to consider a subset of the affine transformations: \textit{linear projections}.  For example, $\pi^1: (x,y) \mapsto (x)$.  Then we have $\pi^1 (I \times \{0\}) = I$.  Note that this projection, when restricted to $I$ (and, more generally, the $x$--axis) is bijective.  Therefore, given our notion of equivalence, we can conclude that $I$ and $I\times \{0\}$ are equivalent.  As an anti--example, consider $I\times I$ (i.e. the unit square in $\R^2$).  While $\pi^1(I\times I) = I$, this mapping is not bijective when restricted to $I$.  Observe that $(0,0) \mapsto (0)$, and $(0,1) \mapsto (0)$, so the projection fails to be injective on $I$.  To indicate why this restricted definition of equivalence may not be suitable for a more robust theory, note that the two line segments $[0,1]$ and $[0,2]$ are not equivalent (given our defintion), even though they are geometrically congruent.  Considering the more general affine transformations resolves this problem, but is not necessary here \todo{mention projective transformations and give a reference for further reading}.

